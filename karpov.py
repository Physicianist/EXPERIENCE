# list []
# set {}
#tuple ()
#dict {Russia": "Moscow", "USA": "Washington"}
#len()
#{} чтобы выводить значения переменных
#<dict>.values() — возвращает список значений
#<dict>.keys() — возвращает список значений
#<dict>.items() — возвращает список таплов вида (ключ, значение)
#name_dict[название ключа] чтобы вывести значение словаря
#<dict>.update({"user3": 8_486_250_00_00, "user5": [8_900_000_12_12, 3_345_233_11_94]})
#<str>.replace(char_1, char_2) — позволяет заменить все символы char_1 в строке на char_2
# Передаем путь with open('responses.txt') as f: text = f.read()
#def function_name( [ позиционные_аргументы,[ *дополнительные_поз_арг,[ именованные_аргументы, [ **доп_имен_арг]]]]):
#def generator_name(arg1, arg2,...): некоторый цикл:yield result
#map(func, iterable)(цикл for для каждой части струкутрыс применением функции)
#odd_list = filter(lambda e: e % 2 == 1, range(10))(возвращает все переменные для которых функци принимает True)
#from functools import reduce # импортируем функцию reduce
#a = [1, 2, 3, 4, 5]
#result = reduce(lambda acc, e: acc + e ** 2, a) Функция, применяемая для агрегации структуры (первый аргумент в reduce), должна принимать 2 аргумента: первый — куда складываем аккумулируемый результат, второй — элемент структуры, по которой происходит итерация.
#sorted(iterable, key=None, reverse = False) key (необязательный) — функция, которая принимает элемент ИТЕРИРУЕМОГО ОБЪЕКТА и возвращает значение, по которому будет производиться сортировка
#def calculate_days(days_passed,weeks_total=12): параметр по умолчанию


#(import math
#math.log10(10))
# matrix.shape[0] - только количество строк
# matrix.shape[1] - только количество столбцов
#matrix.shape #три строки, два столбца
#pd.Series — фактически np.array, но с именами и дополнительными особенностями
#pd.DataFrame (состоит из колонок, каждая из которых — pd.Series) — типичная табличка с данными, датафрейм
#ages = pd.Series([22, 35, 58], name="Age") чтобы создать отдельную СЕРИЮ
#df_ages = ages.to_frame()
#titanic = pd.read_csv("data/titanic.csv")
#titanic.to_csv('second_titanic.csv', index=False)
#titanic.info()
#titanic.iloc[[0, 1]]
#если мы хотим взять не строки, а столбцы, то на первой позиции знак ':'
#такой код возьмёт первый и второй столбец
#titanic.iloc[:, [0, 1]]
##возьмём строки с 10 по 25
#и столбцы с 3 по 5 titanic.iloc[9:25, 2:5]
#(выберем пассажиров, которые плыли либо 2, либо 3 классом
#.isin() принимает на вход список с нужными нам значениями
#class_23 = titanic.loc[titanic["Pclass"].isin([2, 3])]
#иначе можно записать череду условий через '|'
#обратите внимание на скобки
#class_23 = titanic.loc[(titanic["Pclass"] == 2) | (titanic["Pclass"] == 3)])
#.notna(), который возвращает False для всех NaN и True для всего остального. Есть и обратный метод — .isna()
#(#заменим у человека с таким именем возраст на 25 лет titanic.loc[titanic.Name == 'Allen, Mr. William Henry', 'Age'] = 25)
#(на вход принимает словарь
#ключ - название столбца
#значение - функция или имя метода, встроенного в pandas
#можно подать сразу несколько через список
#подсчитаем минимум, максимум и медиану для обоих столбцов
#для возраста ещё подсчитаем коэффициент асимметрии
#для цены билета - среднее
#titanic.agg({'Age': ['min', 'max', 'median', 'skew'],'Fare': ['min', 'max', 'median', 'mean']})
#dop=taxi_renamed[['pickups','borough']].GROUPBY("borough").sum()!!!!!!!!
#dop.sort_values(by='pickups',ascending=False) для датафрейма если SERIES ТО БЕЗ BY!!!!!
#idxmin() – возвращает индекс минимального значения (не само значение, а индекс строки
#.to_frame()
#pd.read_csv('path_to_your.csv', encoding='Windows-1251', sep=';')  !!!!
#query!!!!    product_data.query("title == 'Курс обучения «Эксперт»' and status == 'Завершен'")
#value_counts() Для подсчета количество строк с одинаковыми значениями!!!!!!!!!
#bookings.query('is_canceled==0').country.value_counts().head()!!!!!!
#bookings[['hotel','stays_total_nights']].groupby("hotel").mean()!!!!!!!
#YMC.loc[YMC.groupby('arrival_date_year')['count'].idxmax()]!!!!!!Чтобы вывести самые большие значения для каждого года
#no2_subset.pivot(columns="location", values="value")
#no2.pivot(columns="location", values="value").plot()
#air_quality.pivot_table(  values="value",  index="location",  columns="parameter", aggfunc="mean")
#air_quality = pd.concat([air_quality_pm25, air_quality_no2], axis=0)
#air_quality = pd.merge(air_quality, stations_coord,
#                       how='left',
#                       on='location')
#left_on - как столбец называется в первом датафрейме
#right_on - как во втором
#air_quality = pd.merge(air_quality, air_quality_parameters,
#                       how='left',
#                       left_on='parameter', right_on='id')!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
#
# с заданными x и y: plt.plot([1, 2, 3, 4, 5], [2, 3, 4, 6, 7])
#plt.title("Линейная зависимость y от x")
#plt.xlabel("x") Подпись оси
#plt.grid() Отображение сетки
#plt.plot(x, y, '|--g', label='label') #label присваивает имя этой прямой
#plt.legend()
#plt.legend(loc='upper center')!!!!!!!
#plt.plot(x, y, 'g|--', linewidth=2, markersize=16)
#plt.plot(x, y, '|--g', x, y2, 'rd')  #два x, два y, свои элементы форматирования для каждого графика
#plt.plot(x, y, '|--g', label='Зеленый график')
#plt.plot(x, y2, 'rd', label='Красный график') !!!!!!!!!!!!!
# добавляем мелкие отсечки plt.minorticks_on()
#plt.axis([0, 6, 0, 100])
#plt.ylim(-2, 2) #границы значений по y
#plt.annotate('local max', xy=(2, 1), xytext=(3, 1.5),arrowprops=dict(facecolor='black', shrink=0.05)) Аннотация!!!
#ax1 = plt.subplot2grid(gridsize, (0, 0), colspan=2, rowspan=2) #этот график вверху занимает два ряда и два столбца
#data - наш датафрейм
#x - столбец, для которого мы рисуем гистограмму
#hue - по какому столбцу раскрасить эти гистограммы(по какому стобцу таблицы паскрасить столбцы гистограммы)!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
#col - по какому столбцу разделить эти гистограммы на три системы осей
#sns.displot(data=penguins, x="flipper_length_mm", hue="species", col="species")!!!!!!!!!!!!!
#sns.pairplot(data=penguins, hue="species") #тут будут видны диаграммы рассеяния для всех пар переменных в датафрейме
#sns.catplot(x="day", y="total_bill", hue="smoker", kind="box", data=tips)
#.plot(kind='bar', color='blue', width=1.0)!!!!!!!!Гистограмма через matplotlib
#sns.barplot(x='age', y='count', data=data, palette='viridis')!!!!!!!!!!!!seaborn
#sns.countplot(x='age', data=data)!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
#unique — метод, возвращающий уникальные значения в колонке
#nunique — метод, который считает число уникальных значений в колонке
#split — метод, разбивающий строку на куски и помещающий фрагменты в список
#df = df.rename(columns=lambda c: c.upper().replace('-', '_'))!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
#apply — применяет переданную в него функцию ко всем колонкам вызванного датафрейма. Чтобы применить функцию к одной колонке датафрейма, можно выбрать её перед применением apply
#Иногда вам может захотеться перевести индекс датафрейма в колонку.  Для этого существует метод reset_index(drop=True или False)!!!!!!!!!!!!
#isna — это чудо-метод, с помощью которого можно быстро найти пропущенные значения в датафрейме:
#В figure в figsize подаётся кортеж (как список, только в круглых скобках) с масштабом графика формата (ширина, высота)
#Сохранить график можно с помощью savefig , где аргумент — путь к сохраняемой картинке (желаемое название и формат)
#Все единицы измерения времени можно извлечь сразу с помощью атрибута dt.components!!!!!!!!!!!!!!!!!!!!!!!
#taxi.isna().sum()!!!!!!!!!!!!!!!!!!!количество пропущенных значений в каждом столбце
#value_counts(normalize=True)!!!!!!!!!!!!!!!!!!!!!чтобы получить частоты вместо количтвенных значений
#plt.pie(driver_score_counts['percentage'], labels = driver_score_counts['driver_score'],startangle=90,explode=(0,0.1,0,0,0,0),autopct='%1.2f')!!!!!!!!!!!!!!!!!!!!!!!
#taxi[['start_at', 'end_at', 'arrived_at']] = taxi[['start_at', 'end_at', 'arrived_at']].apply(pd.to_datetime)!!!!!!!!!!!!!!!!!!
#taxi['wait_time'].median().components.minutes Для Timedelta
#query("start_type == 'reserved' and wait_time >@delta")!!!!!!!!!!!!!!!!!!Чтобы сравнить значение в query с переменной
#ads_data = pd.read_csv('ads_data.zip', compression='zip')!!!!!!!!Только если один файл csv в zip
#df.quantile(q=0.75)
#data=data.assign(new=содержимое) !!!!!!!!!!!!!!!Новый способ задания колонок
#fillna(0)!!!!!!!!!!!!!!!!!!!!!!!!Заменяет NAN на 0
#sns.lineplot()
# удалит дубликаты, только если повторяющиеся значения будут в колонках 'Date' и 'Last' df.drop_duplicates(subset=['Date', 'Last'])
#df.loc[df.duplicated()]!!!!!!!!!!!!!Выводит дуюликаты
#str.startswith()!!!!!!!!!!!!
#not_above_35 = titanic.loc[~titanic["Age"] > 35] #строки, для которых это условие даёт False!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
#isin для проверки содержится ли что то в столбце!!!!!!!!!!!!!!!!!!!!!!
#df.to_excel('данные.xlsx', index=False)
#sum(axis=1)!!!!!!!!!!!!!Чтобы суммировать по строкам
#df.rolling(window=2).mean()!!!!!!!!!!!!Чтобы вычисить скользящию функцию,window для размера окна подсчета
#df.rolling(window=3, center=True).mean()///// (0+1+2)/3=1
#rolling(5,min_periods=1)Чтобы избавиься от NAN
#df.ewm(span=2).mean()экспоненциальное среднее
#for num, letter in zip(nums, letters)
#pd.cut(массив значений,список из границ интервало[0,3,5,10],labels=['','','']
#orders_with_sales_team = pd.merge(order_leads, sales_team, on=['Company Id','Company Name']) объединены строки, где совпадают значения во всех указанных в on колонках
#!!!!!!!!!!!!!!!!!!!import plotly.express as px px.line(conversion_df, conversion_df.index, conversion_df['Conversion_rate']) библиотека для создания интерактивных графиков!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
#from my_module import my_function — импортировать функцию my_function, содержащуюся в my_module.py После этого её можно использовать в коде как my_functio
# Приведём колонку money к типу данных float   df = df.astype({'money': 'float'})  !!!!!!!
# Аналогично привели колонку height к типу данных float  df.height = df.height.astype('float') !!!!
# Из датафрейма df убираем колонку Date df = df.drop(columns = 'Date') !!!!!!!!!!
# Из датафрейма df убираем строку с индексом 350 df = df.drop(index = 350)  !!!!!!
#file = open('text.txt') Альтернативный способ открытия файла помимо read_csv
#pattern = re.compile('@([\\w.]+)')!!!!!!!!!!!!!!!!!!!!!!
#pattern.findall(mail)!!!!!!!!!!!!!!!!!!!!!!!!Регулярнык выражения
#\\d — любая цифра (digits)
#\\D — всё, что угодно, кроме цифры
#\\s — любой пробельный символ (spaces)
#\\S — всё, что угодно, кроме пробельного символа
#\\w — любая буква, цифра или _ (words)
#\\W — всё, что угодно, кроме буквы, цифр или _
# . — любой символ
#() обозначают группы символов в паттерне, выводится то что в скобочках
# * — сколько угодно раз (от 0 до бесконечности)
# + — 1 или больше раз
# ? — 0 или 1 раз (то есть или предыдущий символ будет, или нет)
# {} — в скобочках можно указать точное время или диапазон
#df.name.str[:5] первые 5 букв в каждой строчке датафрейма
#df['info'].str.split(',')
#df['info'].str.split(',').str[0]
#df['info'].str.extract('(?P<name>\w+)') (?P...) — говорит питону, что это именованная группа: <name> — имя группы, в данном случае name, extract вытащит его, разобьёт на указанные группы и поместит в новые колонки с именами, как в указанных группах
#pd.DataFrame.filter(items/like/regex, axis)
#items — принимает список с названиями колонок или строк, особой разницы по сравнению с loc нет
#like — принимает строку и возвращает все колонки, где в названии содержится строка, переданная в like
#regex — принимает строку, означающую паттерн РЕ, возвращает все колонки с названиями, которые мэтчатся на паттерн
#axis — параметр для обозначения того, отбираем мы колонки или строки, принимает 'columns' или 'index', по умолчанию фильтрует колонки


# df.column.mode() змеряет моду
#.var() считает дисперсию
#data.std() считает среднеквадратичное отклонение (Показывает реальную среднюю разницу каждого значения и среднего в выборке)sd=корень(суммы/n-1)
#df.quantile(q=0.75)
#ax = sns.boxplot(df.A)
#zscore(df.A) делает стандартизацию(реднее значение будет равно нулю, а стандартное отклонение – равняться 1) x-X/sd
#.sem() Стандартная ошибка среднего (SE) показывает, насколько выборочное среднее отличается от среднего генеральной совокупности. SE при увеличении размера выборки будет стремиться к нулю se=sd/корень(n)
#np.random.normal(mu,sigma,1000) делает нормальное распределение !!!!!!!!!!!!!!
#np.random.choice(генерал совоккуп,размер выборки,False(без повторения выборку))-выборка в 30 элементов
#data.resample(rule='D').column_name.sum()
#В 95% случаев выборочное среднее лежит в интервале +-2se, se=sd/корень(n)!!!!!!!!!!!!!!!!!!!!!!!
# T распределение(мы берем не 1.96сигма а другой в зависимости n-1 и по таблицу находим сколько сигм нужно брать)!!!!!!!!!!!!!!!!!!Когда неизвестно отклоенение в ГС
# t-теста 4 допущения:1)Независимость наблюдений
#                     2)Отсутствие аномальных наблюдений
#                     3)Равенство дисперсий между ГС( не важно)
#                     4)Нормальность обеих ГС( не важно)
# для обнаружения различия либо оно должно быть достаточно большим, либо размер выборок должен быть достаточно большим, либо дисперсия должна быть достаточно маленькой!!!!!!!!!!!!!!!!!!!!!!!!!!!!
#scipy.stats.ttest_ind(data1,data2,equal_var - делать поправку на неравные дисперсии или нет. По умолчанию True)
#pg.ttest(data1, data2)
#sns.pointplot(x="group", #название группирующей переменной
              # y="value", #название переменной, которую мы сравниваем
              # data=df, датафрейм с данными
              # capsize=0.1) #длина засечек на конце "усов" - можно убрать

#Степень свободы в т-распределении то же самое что размер выборки
#Чтобы из перекошенного распределения получить нормальное нужно прологорифмировать np.log()!!!!!!!!!!!!!!!!!!!!!
#ttest_ind если p<0,05 то значения в двух группах сильно различаютсяб, иначе они одинаковые/требование к нормальности данных обеих групп при применении t-теста/На практике t-тест может быть использован для сравнения средних и при ненормальном распределении, особенно на больших выборках и если в данных нет заметных выбросов
#A/В тест показывающий что данные мало различаются не утверждает что так и действительно на самом деле, нужно менять экаперимент!!!!!!!!!!!!!!!!
#scipy.stats.normaltest()!!!!!!!!!!!!!!!!Чирбы проверить является ли распределение нормальное ДЛЯ ДАННЫХ БОЛЬШИХ РАЗМЕРОВ!!!!!!!!!!!!!

#stats.f_oneway(data1,data2,data3)чтобы узнать стоит ли проводить ttest(узнать различаются ли данные), при сильно отличающихся дисперсиях нельзя проводить ттест!!!!!!!!!!!!!!!!!!!!!!!
#data=smf.ols(formula="переменная по котоорой ищут различия(likes)~C(группы в которых мы это ищем) C(button)",data=данные откуда это взято).fit() !!!!!!!!!!!!!!!!!!
#anova_lm(data) выводит степени свободы=(N-1),Residual-внутригрупповая сумма квадратов, сумма квадратов, средняя сумма квадратов, p-value
#pingouin.anova(data,dv=аргумент который мы сравниваем,between=группы которые у нас есть)
#sns.pointplot(x="",y="",data=data)выводит средние с доверительными интервалами!!!!!!!!!!!!!!!!!!!!
#sns.boxplot!!!!!!!!!!!!!!!!!!
#sns.displot(data) ГИСТОГРАММА !!!!!!!!!!!!!
#ss.shapiro(data)тестируем на нормальность, если p>0,05 то нормальные НЕ ПОДХОДИТ ДЛЯ ВЫБОРОК БОЛЬШИХ РАЗМЕРОВ!!!!!!!!!!!!!!!!!!!
#sm.qqplot(группа,line='r') Точки должны лежать на одной прмой в иделае (не сильно отбиваться от нее) ТЕСТ НА НОРМАЛЬНОСТЬ!!!!!!!!
#pg.qqplot(группа)такой же грфик только с доверительными интервалами
#ss.levene(группф1,группа2,....) ТЕСТИРУЕМ РАЗЛИЧИЕ В ДИСПЕРСИЯХ!!!!!!!!!!!!!!!! Нулевая гипотеза то что дисперсии одинаковы то есть p>0,05
#pg.welch_anova (data,dv,between)t-критерий УЭЛЧА(тот же ttest,только здесь задано что у нас разные дисперсии)
#Дисперсионный анализ нужен из за проблемы мноественных значений(т.е. вероятность ошибиться 5%,если таких испытний 3,то вероятность не ошибиться 0.95^3)!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
#comb(3,2)-биномиальный коэффицент(в данном случае показывает сколько пар можно создать из 3)!!!!!!!!!!!!!!!!!!!!!!
#Поправка Бонферрони-берем порог значимости(0.05) и делим его на число попарных сравнений.Получаем новый порог значимости. Очень консервативно
#pg.pairwise_ttests(data,dv,between)попарный ttest
#pg.pairwise_ttests(data,dv,between,padjust="bonf")попарный ttest с поправкой бонферони!!!!
#pg.pairwise_tukey(data,dv,between)попарный ttest с поправкой TUKE:pg.pairwise_tukey(data=task_1,dv='events',between='group')
#pg.pairwise_gameshowell(data,dv,between)попарный ttest с поправкой на различия дисперсий!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
#smf.ols("переменная по котоорой ищут различия(likes)~C(группы в которых мы это ищем) C(button)+C(dop_group)",data=данные откуда это взято).fit() многофакторный анализ без учета свзяи между C(button)+C(dop_group) !!!!!!!!!!!!!!!!!!!!!!!!
#anova_lm()
#smf.ols("переменная по котоорой ищут различия(likes)~C(группы в которых мы это ищем) C(button)+C(dop_group)+C(button):C(dop_group)",data=данные откуда это взято).fit() многофакторный анализ c учетом свзяи между C(button)+C(dop_group) !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
#pingouin.anova(data,dv=аргумент который мы сравниваем,between=["группа1","группа2"]) ТО ЖЕ САМОЕ ТОЛЬКО ЧЕРЕЗ PG
#task_2['combine']=task_2['group']+'/'+task_2['segment']!!!!!!!!!!!!!!!!
#Кэф Пирсона показывает наличие линейной зависимости неустойчи к выбросам
#Кэф Спирмена более устойчив к выбросам( ранк переменных)
#sns.scatterplot(data=data,x='',y='')!!!!!!!!!!!!!!
#scipy.stats.pearsonr(data.x,data.y) Выводит коэффицент кореляции,p-value,показывающий если p<0.05 кореляция стат значимо в ГС то есть отклонем нулевую гипотезу о том что корелляция =0 в ГС!!!!!!!!!!!!!!!!!!!!!!!!
#scipy.stats.spearmanr(data.x,data.y)
#data_frame.corr()ВЫВОДИТ КОЭФФИЦЕНТ КОРИЛЯЦИИ КАЖДОГО С КАЖДЫМ СТОЛБЦОМ!!!!!!!!!!!!!!!!!!!!!!!
#КОЭФФИЦЕНТ КОРИЛЯЦИИ ТОЛЬКО ГОВОРИТ О НАЛИЧИИ ВЗАИМОСВЯЗИ И НИЧЕГО НЕ ГОВОРИТ О ПРИЧИН_СЛЕДСТВИИ!!!!!!!!!!!!
#sns.regplot(x='',y='',data=data) АПРОКСИМИРУЕТ ЛИНЕЙНОЙ ЗАВИСИМОСТЬЮ(РЕГРЕССИОННЫЙ АНАЛИЗ)!!!!!!!!!!!!!!!!!!!!!!!!!
#Множественный регресионный анализ:
#sm.OLS(y, X).fit().summary() Задаем модель
#y = penguins["body_mass_g"],X = penguins[["bill_length_mm", "flipper_length_mm", "species"]]
#X = sm.add_constant(X) чтобы задать свободный член
#X = pd.get_dummies(X, drop_first=True) #этот аргумент необходим! Убрать категориальные переменные, затем они буут считаться относительной выкинутой.
#model_sklrn = sm.OLS(endog=y, exog=X).fit()
#model_sklrn.summary() Считаем модель

#коэффициент детерминации (R^2)— это квадрат коэффициента корреляции Пирсона и Если R2 приближается к 1, это указывает на высокую степень соответствия данных модели!!!!!!!!!!!
#скорректированный R^2 (Adjusted R^2) Так как если взять много переменных которые никак не связаны друг с другом то R^2 все равно будет расти!!!!!
#КОЭФФИЦЕНТ КОРИЛЯЦИИ ЛИШЬ ГОВОРИТ СИЛЬНО ЛИ СВЯЗАНЫ ПЕРЕМЕННЫЕ
#results = smf.ols('Y ~ X', data).fit() ДЛЯ МНК(ЛИНЕЙНОЙ АПРОКСИМАЦИИ)
#print(results.summary())
#НЕНОРМАЛЬНОЕ РАСПРЕДЕЛЕНИЕ=>УДАЛИТЬ ВЫБРОСЫ;ПРОЛОГАРИФМИРОВАТЬ;НЕПАРАМЕТРИКА;BOOTSTRAP
#BOOTSTRAP!!!!!!!!!!!!!!!!!!!!!!!!Мы имитируем ситуацию проведения одного и того же эксперимента много-много раз, считаем для каждого симулированного эксперимента нужную метрику и уже по распределению этой метрики считаем доверительный интервал
#sample_data = df.sample(frac=1, replace=True)
# считаем показатель - в данном случае медиану
#sample_median = sample_data.value.median()

#scipy
#scipy.stats.bootstrap(data-в виде списка, method — алгоритм расчёта доверительного интервала, n_resamples — количество симуляций, confidence_level — уровень значимости, по умолчанию 95%, statistic — функция, которая считает нужный нам параметр. Если на вход идёт две или больше групп, то функция также должна принимать на вход два или больше аргументов и возвращать какое-то одно число, )

#pingouin
#pingouin.compute_bootci()

#Критерий Хи-квадрат используется для проверки того, соответствует ли категориальная случайная величина выбранному распределению
# условия:
#Все наблюдения независимы
#Количество наблюдений в каждой ячейке от 10 и более. Для наблюдений от 5 до 10 часто применяют т.н. поправку Йетса.
#chisquare В scipy функция проводит этот тест, принимая на вход наблюдаемые и ожидаемые в соответствии с нулевой гипотезой значения величины

#Git
#ssh-keygen -t rsa -b 4096-для генерации ssh ключа( public часть нужно скопировать на GitHub)
#cat чтобы посмотреть файл
#ls посмотреть что содержится в директории
#rm чтобы удалить файл
#git clone и имя ссылки с GitHub
#"""1"""" git add название файла (чтобы git начал следить за ними)
#git status
#"""2""""l (-m, чтобы убрать сообщения после команды commit)
#git diff (показывает разницу между сохранениями)
# git log (показывает commits и комментарии к ним которые мы оставляли)
# """3"""" git push (отправляет все коммиты и последний файфл на сервер GitHub)
# git pull (чтобы сохранить к себе все изменения которые есть в GitHub)
# git branch ( чтобы создать новые ветки (обычно копии нашего проекта, в которых можно изменять что-либо, а в исходном файле ничего не изменится))
# git chekout название ветки ( переключается на ветку)
# git push origin название ветки (Чтобы внести в основной проект изменения, потом переходим по ссылке, которая высветилась, затем
# git merge название ветки ( накатили все изменения в ветку где мы щас из ветки название ветки)
# git branch -D название ветки (удаляет ветку)
#
#


import scipy

import seaborn as sns
import matplotlib.pyplot as plt

import pandas as pd
import numpy as np
# import xlsxwriter as xlsx
# import plotly.express as px
import re

import statsmodels.api as sm
import statsmodels.formula.api as smf
from fontTools.misc.fixedTools import floatToFixedToFloat
from statsmodels.stats.api import anova_lm
from statsmodels.stats.multicomp import (MultiComparison,pairwise_tukeyhsd)

import pingouin as pg  #для дисперсионного анализа

from scipy.special import comb


from scipy import stats

from pandas import read_csv
from scipy.stats import zscore
from scipy.stats import ttest_ind #!!!!!!!!!!!!
from numpy import zeros
# from scipy import special !!!!!!!!!!!!!!!!!
# special.comb(n, k) #биномиальный коэффициент!!!!!!!!!!!!!!!!!!!!

